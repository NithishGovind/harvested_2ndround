{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv11 Weed Detection - Complete Pipeline\n",
    "## Dataset: [Weeds Detection](https://www.kaggle.com/datasets/swish9/weeds-detection)\n",
    "\n",
    "This notebook implements **end-to-end object detection** for crop and weed classification using YOLOv11 with:\n",
    "- âœ… Data exploration & preprocessing\n",
    "- âœ… YOLOv11 training with hyperparameter optimization\n",
    "- âœ… Comprehensive evaluation metrics\n",
    "- âœ… Per-class performance analysis\n",
    "- âœ… Prediction visualization\n",
    "- âœ… Model export for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install ultralytics opencv-python pillow pyyaml matplotlib scikit-learn -q\n",
    "print(\"âœ“ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, precision_recall_curve,\n",
    "    average_precision_score, f1_score\n",
    ")\n",
    "\n",
    "# Set style and random seeds\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA EXPLORATION & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for Kaggle environment\n",
    "DATASET_PATH = '/kaggle/input/weeds-detection/dataset'\n",
    "WORKING_DIR = '/kaggle/working'\n",
    "OUTPUT_DIR = os.path.join(WORKING_DIR, 'yolo_output')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset structure\n",
    "TRAIN_IMAGES = os.path.join(DATASET_PATH, 'images/train')\n",
    "VAL_IMAGES = os.path.join(DATASET_PATH, 'images/val')\n",
    "TEST_IMAGES = os.path.join(DATASET_PATH, 'images/test')\n",
    "\n",
    "TRAIN_LABELS = os.path.join(DATASET_PATH, 'labels/train')\n",
    "VAL_LABELS = os.path.join(DATASET_PATH, 'labels/val')\n",
    "TEST_LABELS = os.path.join(DATASET_PATH, 'labels/test')\n",
    "\n",
    "# Class mapping\n",
    "CLASSES = ['Crop', 'Weed']\n",
    "\n",
    "print(f\"ðŸ“ Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"ðŸ“ Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structure\n",
    "def explore_dataset():\n",
    "    \"\"\"Analyze dataset structure and statistics\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    for split, (img_dir, lbl_dir) in [\n",
    "        ('train', (TRAIN_IMAGES, TRAIN_LABELS)),\n",
    "        ('val', (VAL_IMAGES, VAL_LABELS)),\n",
    "        ('test', (TEST_IMAGES, TEST_LABELS))\n",
    "    ]:\n",
    "        if os.path.exists(img_dir):\n",
    "            img_count = len(os.listdir(img_dir))\n",
    "            lbl_count = len(os.listdir(lbl_dir)) if os.path.exists(lbl_dir) else 0\n",
    "            stats[split] = {'images': img_count, 'labels': lbl_count}\n",
    "    \n",
    "    return stats\n",
    "\n",
    "dataset_stats = explore_dataset()\n",
    "print(\"\\nðŸ“Š Dataset Statistics:\")\n",
    "for split, counts in dataset_stats.items():\n",
    "    print(f\"  {split.upper()}: {counts['images']} images, {counts['labels']} labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "def analyze_labels(label_dir, split_name):\n",
    "    \"\"\"Analyze class distribution in labels\"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    if not os.path.exists(label_dir):\n",
    "        return None\n",
    "    \n",
    "    for label_file in os.listdir(label_dir):\n",
    "        if label_file.endswith('.txt'):\n",
    "            with open(os.path.join(label_dir, label_file), 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        parts = line.strip().split()\n",
    "                        class_id = int(parts[0])\n",
    "                        class_counts[CLASSES[class_id]] += 1\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "print(\"\\nðŸ“ˆ Class Distribution:\")\n",
    "all_class_stats = {}\n",
    "for split, (split_name, lbl_dir) in [\n",
    "    ('train', ('TRAIN', TRAIN_LABELS)),\n",
    "    ('val', ('VAL', VAL_LABELS)),\n",
    "    ('test', ('TEST', TEST_LABELS))\n",
    "]:\n",
    "    counts = analyze_labels(lbl_dir, split)\n",
    "    all_class_stats[split] = counts\n",
    "    if counts:\n",
    "        total = sum(counts.values())\n",
    "        print(f\"\\n  {split_name}:\")\n",
    "        for cls, count in sorted(counts.items()):\n",
    "            pct = (count / total) * 100\n",
    "            print(f\"    {cls}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images with annotations\n",
    "def visualize_sample_images(img_dir, lbl_dir, num_samples=3):\n",
    "    \"\"\"Visualize sample images with bounding boxes\"\"\"\n",
    "    img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))])[:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(16, 5))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, img_file in enumerate(img_files):\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load labels\n",
    "        label_file = img_file.replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "        label_path = os.path.join(lbl_dir, label_file)\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            h, w = img.shape[:2]\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    class_id = int(parts[0])\n",
    "                    cx, cy, bw, bh = map(float, parts[1:])\n",
    "                    \n",
    "                    # Convert YOLO format to pixel coordinates\n",
    "                    x1 = int((cx - bw/2) * w)\n",
    "                    y1 = int((cy - bh/2) * h)\n",
    "                    x2 = int((cx + bw/2) * w)\n",
    "                    y2 = int((cy + bh/2) * h)\n",
    "                    \n",
    "                    class_name = CLASSES[class_id]\n",
    "                    color = (0, 255, 0) if class_id == 0 else (0, 0, 255)  # Green: Crop, Red: Weed\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(img, class_name, (x1, max(y1-5, 0)),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'Train Sample: {img_file}')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'sample_images.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ“ Sample images saved!\")\n",
    "\n",
    "visualize_sample_images(TRAIN_IMAGES, TRAIN_LABELS, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CREATE YOLO CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data.yaml for YOLO\n",
    "data_config = {\n",
    "    'path': DATASET_PATH,\n",
    "    'train': TRAIN_IMAGES,\n",
    "    'val': VAL_IMAGES,\n",
    "    'test': TEST_IMAGES,\n",
    "    'nc': len(CLASSES),\n",
    "    'names': {i: cls for i, cls in enumerate(CLASSES)}\n",
    "}\n",
    "\n",
    "config_path = os.path.join(OUTPUT_DIR, 'data.yaml')\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"âœ“ YOLO configuration saved to: {config_path}\")\n",
    "print(\"\\nYAML Configuration:\")\n",
    "with open(config_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and train YOLOv11\n",
    "print(\"ðŸš€ Loading YOLOv11 model...\")\n",
    "model = YOLO('yolov11n.pt')  # nano model for faster training\n",
    "# Alternative models: 'yolov11s.pt' (small), 'yolov11m.pt' (medium), \n",
    "#                    'yolov11l.pt' (large), 'yolov11x.pt' (extra-large)\n",
    "\n",
    "print(\"\\nðŸ“š Starting training...\")\n",
    "results = model.train(\n",
    "    data=config_path,\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,  # Adjust based on GPU memory (larger = faster but more memory)\n",
    "    device=0,  # GPU device ID (0 for first GPU)\n",
    "    patience=10,  # Early stopping patience\n",
    "    save=True,\n",
    "    verbose=True,\n",
    "    # Data augmentation\n",
    "    augment=True,\n",
    "    flipud=0.5,\n",
    "    fliplr=0.5,\n",
    "    mosaic=1.0,\n",
    "    # Optimization\n",
    "    optimizer='auto',\n",
    "    lr0=0.01,\n",
    "    lrf=0.01,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training results summary\n",
    "print(\"\\nðŸ“Š Training Results Summary:\")\n",
    "results_dict = results.results_dict\n",
    "\n",
    "# Display key metrics\n",
    "key_metrics = ['metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'metrics/precision(B)', 'metrics/recall(B)']\n",
    "for metric in key_metrics:\n",
    "    if metric in results_dict:\n",
    "        print(f\"  {metric}: {results_dict[metric]:.4f}\")\n",
    "\n",
    "# Save training summary\n",
    "with open(os.path.join(OUTPUT_DIR, 'training_summary.json'), 'w') as f:\n",
    "    json.dump(results_dict, f, indent=4)\n",
    "\n",
    "print(f\"\\nâœ“ Training summary saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MODEL VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate on validation set\n",
    "print(\"ðŸ” Validating on validation set...\")\n",
    "val_metrics = model.val()\n",
    "\n",
    "print(\"\\nâœ“ Validation Metrics:\")\n",
    "print(f\"  mAP50: {val_metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP50-95: {val_metrics.box.map:.4f}\")\n",
    "print(f\"  Precision: {val_metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall: {val_metrics.box.mr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. INFERENCE & TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test set\n",
    "print(\"ðŸŽ¯ Running inference on test set...\")\n",
    "test_results = model.predict(source=TEST_IMAGES, conf=0.5, save=True, verbose=False)\n",
    "print(f\"âœ“ Test predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed test set evaluation\n",
    "def evaluate_test_set():\n",
    "    \"\"\"Comprehensive evaluation on test set\"\"\"\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    test_img_files = sorted([f for f in os.listdir(TEST_IMAGES) \n",
    "                            if f.endswith(('.jpg', '.png'))])\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Evaluating {len(test_img_files)} test images...\")\n",
    "    \n",
    "    for img_file in test_img_files:\n",
    "        img_path = os.path.join(TEST_IMAGES, img_file)\n",
    "        \n",
    "        # Model prediction\n",
    "        results = model.predict(img_path, conf=0.5, verbose=False)\n",
    "        \n",
    "        # Ground truth\n",
    "        label_file = img_file.replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "        label_path = os.path.join(TEST_LABELS, label_file)\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            img = cv2.imread(img_path)\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            # Extract ground truth boxes\n",
    "            gt_boxes = []\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        cx, cy, bw, bh = map(float, parts[1:5])\n",
    "                        gt_boxes.append({\n",
    "                            'class_id': class_id,\n",
    "                            'x1': (cx - bw/2) * w,\n",
    "                            'y1': (cy - bh/2) * h,\n",
    "                            'x2': (cx + bw/2) * w,\n",
    "                            'y2': (cy + bh/2) * h\n",
    "                        })\n",
    "            \n",
    "            # Extract predictions\n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    predictions.append({\n",
    "                        'img_file': img_file,\n",
    "                        'class_id': int(box.cls[0]),\n",
    "                        'confidence': float(box.conf[0]),\n",
    "                        'x1': float(box.xyxy[0][0]),\n",
    "                        'y1': float(box.xyxy[0][1]),\n",
    "                        'x2': float(box.xyxy[0][2]),\n",
    "                        'y2': float(box.xyxy[0][3])\n",
    "                    })\n",
    "            \n",
    "            ground_truths.append({\n",
    "                'img_file': img_file,\n",
    "                'boxes': gt_boxes\n",
    "            })\n",
    "    \n",
    "    return predictions, ground_truths\n",
    "\n",
    "predictions, ground_truths = evaluate_test_set()\n",
    "predictions_df = pd.DataFrame(predictions) if predictions else pd.DataFrame()\n",
    "\n",
    "print(f\"âœ“ Extracted {len(predictions)} predictions\")\n",
    "print(f\"âœ“ Extracted {len(ground_truths)} ground truth images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IoU and detection metrics\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union\"\"\"\n",
    "    x1_inter = max(box1['x1'], box2['x1'])\n",
    "    y1_inter = max(box1['y1'], box2['y1'])\n",
    "    x2_inter = min(box1['x2'], box2['x2'])\n",
    "    y2_inter = min(box1['y2'], box2['y2'])\n",
    "    \n",
    "    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "    \n",
    "    box1_area = (box1['x2'] - box1['x1']) * (box1['y2'] - box1['y1'])\n",
    "    box2_area = (box2['x2'] - box2['x1']) * (box2['y2'] - box2['y1'])\n",
    "    \n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def match_boxes(predictions_df, ground_truths, iou_threshold=0.5):\n",
    "    \"\"\"Match predicted boxes to ground truth\"\"\"\n",
    "    tp = 0  # True positives\n",
    "    fp = 0  # False positives\n",
    "    fn = 0  # False negatives\n",
    "    \n",
    "    for gt in ground_truths:\n",
    "        img_file = gt['img_file']\n",
    "        img_preds = predictions_df[predictions_df['img_file'] == img_file]\n",
    "        \n",
    "        matched = set()\n",
    "        for gt_box in gt['boxes']:\n",
    "            best_iou = 0\n",
    "            best_pred_idx = -1\n",
    "            \n",
    "            for pred_idx, (_, pred) in enumerate(img_preds.iterrows()):\n",
    "                if pred_idx in matched:\n",
    "                    continue\n",
    "                \n",
    "                iou = calculate_iou(gt_box, {\n",
    "                    'x1': pred['x1'], 'y1': pred['y1'],\n",
    "                    'x2': pred['x2'], 'y2': pred['y2']\n",
    "                })\n",
    "                \n",
    "                if iou > best_iou and pred['class_id'] == gt_box['class_id']:\n",
    "                    best_iou = iou\n",
    "                    best_pred_idx = pred_idx\n",
    "            \n",
    "            if best_iou >= iou_threshold and best_pred_idx >= 0:\n",
    "                tp += 1\n",
    "                matched.add(best_pred_idx)\n",
    "            else:\n",
    "                fn += 1\n",
    "        \n",
    "        fp += len(img_preds) - len(matched)\n",
    "    \n",
    "    return tp, fp, fn\n",
    "\n",
    "# Calculate metrics\n",
    "tp, fp, fn = match_boxes(predictions_df, ground_truths, iou_threshold=0.5)\n",
    "\n",
    "print(f\"\\nâœ“ Detection Metrics (IoU=0.5):\")\n",
    "print(f\"  True Positives (TP): {tp}\")\n",
    "print(f\"  False Positives (FP): {fp}\")\n",
    "print(f\"  False Negatives (FN): {fn}\")\n",
    "\n",
    "if (tp + fp) > 0:\n",
    "    precision = tp / (tp + fp)\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "else:\n",
    "    precision = 0\n",
    "\n",
    "if (tp + fn) > 0:\n",
    "    recall = tp / (tp + fn)\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "else:\n",
    "    recall = 0\n",
    "\n",
    "if precision + recall > 0:\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print(f\"  F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "def calculate_per_class_metrics():\n",
    "    \"\"\"Calculate metrics per class\"\"\"\n",
    "    metrics_per_class = {}\n",
    "    \n",
    "    for class_id, class_name in enumerate(CLASSES):\n",
    "        class_preds = predictions_df[predictions_df['class_id'] == class_id] if len(predictions_df) > 0 else pd.DataFrame()\n",
    "        \n",
    "        # Count GT boxes for this class\n",
    "        gt_count = sum(\n",
    "            len([b for b in gt['boxes'] if b['class_id'] == class_id])\n",
    "            for gt in ground_truths\n",
    "        )\n",
    "        \n",
    "        metrics_per_class[class_name] = {\n",
    "            'predictions': len(class_preds),\n",
    "            'ground_truth': gt_count,\n",
    "            'avg_confidence': class_preds['confidence'].mean() if len(class_preds) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    return metrics_per_class\n",
    "\n",
    "per_class_metrics = calculate_per_class_metrics()\n",
    "\n",
    "print(f\"\\nðŸ“Š Per-Class Metrics:\")\n",
    "for class_name, metrics in per_class_metrics.items():\n",
    "    print(f\"\\n  {class_name}:\")\n",
    "    print(f\"    Predictions: {metrics['predictions']}\")\n",
    "    print(f\"    Ground Truth: {metrics['ground_truth']}\")\n",
    "    print(f\"    Avg Confidence: {metrics['avg_confidence']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COMPREHENSIVE EVALUATION PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence distribution and evaluation plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Confidence distribution\n",
    "if len(predictions_df) > 0:\n",
    "    axes[0, 0].hist(predictions_df['confidence'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0, 0].set_xlabel('Confidence Score', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0, 0].set_title('Distribution of Prediction Confidence', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predictions per class\n",
    "if len(predictions_df) > 0:\n",
    "    class_counts = predictions_df['class_id'].value_counts()\n",
    "    class_names = [CLASSES[int(cid)] for cid in class_counts.index]\n",
    "    colors = ['green' if cn == 'Crop' else 'red' for cn in class_names]\n",
    "    axes[0, 1].bar(class_names, class_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_ylabel('Number of Detections', fontsize=11)\n",
    "    axes[0, 1].set_title('Detections per Class', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(class_counts.values):\n",
    "        axes[0, 1].text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: Detection metrics\n",
    "metrics_names = ['TP', 'FP', 'FN']\n",
    "metrics_values = [tp, fp, fn]\n",
    "colors_metrics = ['green', 'red', 'orange']\n",
    "axes[1, 0].bar(metrics_names, metrics_values, color=colors_metrics, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_ylabel('Count', fontsize=11)\n",
    "axes[1, 0].set_title('Detection Metrics Summary (IoU=0.5)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(metrics_values):\n",
    "    axes[1, 0].text(i, v + max(metrics_values)*0.02, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 4: Per-class comparison\n",
    "class_names_full = list(per_class_metrics.keys())\n",
    "pred_counts = [per_class_metrics[cn]['predictions'] for cn in class_names_full]\n",
    "gt_counts = [per_class_metrics[cn]['ground_truth'] for cn in class_names_full]\n",
    "\n",
    "x = np.arange(len(class_names_full))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, pred_counts, width, label='Predictions', alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[1, 1].bar(x + width/2, gt_counts, width, label='Ground Truth', alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Count', fontsize=11)\n",
    "axes[1, 1].set_title('Predictions vs Ground Truth', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(class_names_full)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'evaluation_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Evaluation plots saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on test images\n",
    "def visualize_predictions(num_samples=5):\n",
    "    \"\"\"Visualize model predictions on test images\"\"\"\n",
    "    test_imgs = sorted([f for f in os.listdir(TEST_IMAGES) \n",
    "                       if f.endswith(('.jpg', '.png'))])[:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(14, 5*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, img_file in enumerate(test_imgs):\n",
    "        img_path = os.path.join(TEST_IMAGES, img_file)\n",
    "        \n",
    "        # Ground truth\n",
    "        img_gt = cv2.imread(img_path)\n",
    "        img_gt = cv2.cvtColor(img_gt, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img_gt.shape[:2]\n",
    "        \n",
    "        label_file = img_file.replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "        label_path = os.path.join(TEST_LABELS, label_file)\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    class_id = int(parts[0])\n",
    "                    cx, cy, bw, bh = map(float, parts[1:5])\n",
    "                    x1, y1 = int((cx - bw/2) * w), int((cy - bh/2) * h)\n",
    "                    x2, y2 = int((cx + bw/2) * w), int((cy + bh/2) * h)\n",
    "                    color = (0, 255, 0) if class_id == 0 else (255, 0, 0)\n",
    "                    cv2.rectangle(img_gt, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(img_gt, CLASSES[class_id], (x1, max(y1-5, 0)),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        axes[idx, 0].imshow(img_gt)\n",
    "        axes[idx, 0].set_title(f'Ground Truth: {img_file}', fontweight='bold')\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        # Predictions\n",
    "        results = model.predict(img_path, conf=0.5, verbose=False)\n",
    "        img_pred = cv2.imread(img_path)\n",
    "        img_pred = cv2.cvtColor(img_pred, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                class_id = int(box.cls[0])\n",
    "                conf = float(box.conf[0])\n",
    "                color = (0, 255, 0) if class_id == 0 else (255, 0, 0)\n",
    "                cv2.rectangle(img_pred, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(img_pred, f'{CLASSES[class_id]} {conf:.2f}',\n",
    "                           (x1, max(y1-5, 0)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        axes[idx, 1].imshow(img_pred)\n",
    "        axes[idx, 1].set_title(f'Predictions: {img_file}', fontweight='bold')\n",
    "        axes[idx, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'predictions_visualization.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ“ Predictions visualization saved!\")\n",
    "\n",
    "visualize_predictions(num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FINAL EVALUATION REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "report = f\"\"\"\n",
    "{'='*90}\n",
    "YOLOV11 WEED DETECTION - FINAL EVALUATION REPORT\n",
    "{'='*90}\n",
    "\n",
    "DATASET INFORMATION\n",
    "{'-'*90}\n",
    "Training Images: {dataset_stats.get('train', {}).get('images', 'N/A')}\n",
    "Validation Images: {dataset_stats.get('val', {}).get('images', 'N/A')}\n",
    "Test Images: {dataset_stats.get('test', {}).get('images', 'N/A')}\n",
    "Classes: {', '.join(CLASSES)}\n",
    "\n",
    "TRAINING CONFIGURATION\n",
    "{'-'*90}\n",
    "Model: YOLOv11 Nano\n",
    "Epochs: 50\n",
    "Batch Size: 16\n",
    "Image Size: 640Ã—640\n",
    "Optimizer: Auto (with lr scheduling)\n",
    "Learning Rate: 0.01 â†’ 0.01\n",
    "Augmentation: Enabled (Flip, Mosaic)\n",
    "Early Stopping: Patience=10\n",
    "\n",
    "DETECTION METRICS (IoU=0.5)\n",
    "{'-'*90}\n",
    "True Positives (TP): {tp}\n",
    "False Positives (FP): {fp}\n",
    "False Negatives (FN): {fn}\n",
    "Precision: {tp / (tp + fp):.4f if (tp + fp) > 0 else 'N/A'}\n",
    "Recall: {tp / (tp + fn):.4f if (tp + fn) > 0 else 'N/A'}\n",
    "F1-Score: {2 * (tp / (tp + fp)) * (tp / (tp + fn)) / ((tp / (tp + fp)) + (tp / (tp + fn))):.4f if (tp + fp) > 0 and (tp + fn) > 0 else 'N/A'}\n",
    "\n",
    "PER-CLASS PERFORMANCE\n",
    "{'-'*90}\n",
    "\"\"\"\n",
    "\n",
    "for class_name, metrics in per_class_metrics.items():\n",
    "    report += f\"\\n{class_name}:\"\n",
    "    report += f\"\\n  Total Predictions: {metrics['predictions']}\"\n",
    "    report += f\"\\n  Ground Truth Count: {metrics['ground_truth']}\"\n",
    "    report += f\"\\n  Average Confidence: {metrics['avg_confidence']:.4f}\\n\"\n",
    "\n",
    "report += f\"\"\"\nINFERENCE STATISTICS\n",
    "{'-'*90}\n",
    "Total Predictions Made: {len(predictions_df)}\n",
    "Unique Test Images Processed: {predictions_df['img_file'].nunique() if len(predictions_df) > 0 else 0}\n",
    "Average Prediction Confidence: {predictions_df['confidence'].mean():.4f if len(predictions_df) > 0 else 'N/A'}\n",
    "Min Confidence: {predictions_df['confidence'].min():.4f if len(predictions_df) > 0 else 'N/A'}\n",
    "Max Confidence: {predictions_df['confidence'].max():.4f if len(predictions_df) > 0 else 'N/A'}\n",
    "\n",
    "OUTPUT FILES\n",
    "{'-'*90}\n",
    "âœ“ Model weights: {OUTPUT_DIR}/runs/detect/train*/weights/best.pt\n",
    "âœ“ Training plots: {OUTPUT_DIR}/runs/detect/train*/results.png\n",
    "âœ“ Evaluation metrics: {OUTPUT_DIR}/evaluation_metrics.png\n",
    "âœ“ Predictions visualization: {OUTPUT_DIR}/predictions_visualization.png\n",
    "âœ“ Sample images: {OUTPUT_DIR}/sample_images.png\n",
    "âœ“ Training summary: {OUTPUT_DIR}/training_summary.json\n",
    "âœ“ ONNX model: {OUTPUT_DIR}/runs/detect/train*/weights/best.onnx\n",
    "\n",
    "RECOMMENDATIONS FOR IMPROVEMENT\n",
    "{'-'*90}\n",
    "1. **Class Imbalance**: If classes are imbalanced, consider:\n",
    "   - Using weighted loss functions\n",
    "   - Data augmentation (more samples for minority class)\n",
    "   - Stratified sampling in data splits\n",
    "\n",
    "2. **Model Architecture**: Try larger models for better accuracy:\n",
    "   - Current: yolov11n.pt (Nano)\n",
    "   - Faster: yolov11s.pt (Small)\n",
    "   - Better: yolov11m.pt (Medium)\n",
    "   - Best: yolov11l.pt (Large) or yolov11x.pt (Extra-Large)\n",
    "\n",
    "3. **Data Augmentation**: Increase diversity:\n",
    "   - Rotation, Shear, Perspective transforms\n",
    "   - Color jittering and Gaussian blur\n",
    "   - CutMix and MixUp augmentations\n",
    "\n",
    "4. **Hyperparameter Tuning**:\n",
    "   - Adjust learning rate and schedule\n",
    "   - Increase batch size if GPU memory allows\n",
    "   - Fine-tune optimizer (SGD vs Adam)\n",
    "\n",
    "5. **Post-Processing**:\n",
    "   - Apply NMS (Non-Maximum Suppression) tuning\n",
    "   - Confidence threshold optimization\n",
    "   - IoU threshold calibration\n",
    "\n",
    "6. **Deployment**: Export to production formats:\n",
    "   - ONNX for cross-platform inference\n",
    "   - TensorFlow Lite for mobile/edge devices\n",
    "   - OpenVINO for Intel hardware optimization\n",
    "\n",
    "{'='*90}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open(os.path.join(OUTPUT_DIR, 'evaluation_report.txt'), 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nâœ“ Report saved to: {OUTPUT_DIR}/evaluation_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. MODEL EXPORT FOR DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to multiple formats for deployment\n",
    "print(\"\\nðŸ“¦ Exporting model to multiple formats...\\n\")\n",
    "\n",
    "# ONNX export (cross-platform inference)\n",
    "print(\"Exporting to ONNX...\")\n",
    "onnx_path = model.export(format='onnx')\n",
    "print(f\"âœ“ ONNX model: {onnx_path}\")\n",
    "\n",
    "# TorchScript export\n",
    "print(\"\\nExporting to TorchScript...\")\n",
    "torchscript_path = model.export(format='torchscript')\n",
    "print(f\"âœ“ TorchScript model: {torchscript_path}\")\n",
    "\n",
    "# TensorFlow export (optional - takes longer)\n",
    "# print(\"\\nExporting to TensorFlow...\")\n",
    "# tf_path = model.export(format='tensorflow')\n",
    "# print(f\"âœ“ TensorFlow model: {tf_path}\")\n",
    "\n",
    "print(\"\\nâœ“ Model export completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"âœ“ YOLOV11 WEED DETECTION PIPELINE COMPLETE!\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\nðŸ“ All outputs saved to: {OUTPUT_DIR}\")\n",
    "print(f\"\\nðŸ“‹ Key outputs:\")\n",
    "print(f\"  â€¢ Best model weights: {OUTPUT_DIR}/runs/detect/train/weights/best.pt\")\n",
    "print(f\"  â€¢ Evaluation report: {OUTPUT_DIR}/evaluation_report.txt\")\n",
    "print(f\"  â€¢ Metrics plot: {OUTPUT_DIR}/evaluation_metrics.png\")\n",
    "print(f\"  â€¢ Predictions visualization: {OUTPUT_DIR}/predictions_visualization.png\")\n",
    "print(f\"  â€¢ ONNX model: {onnx_path}\")\n",
    "print(f\"\\nðŸŽ¯ Detection Performance:\")\n",
    "print(f\"  â€¢ Precision: {tp / (tp + fp):.4f}\" if (tp + fp) > 0 else \"  â€¢ Precision: N/A\")\n",
    "print(f\"  â€¢ Recall: {tp / (tp + fn):.4f}\" if (tp + fn) > 0 else \"  â€¢ Recall: N/A\")\n",
    "print(f\"  â€¢ F1-Score: {2 * (tp / (tp + fp)) * (tp / (tp + fn)) / ((tp / (tp + fp)) + (tp / (tp + fn))):.4f}\" if (tp + fp) > 0 and (tp + fn) > 0 else \"  â€¢ F1-Score: N/A\")\n",
    "print(f\"\\nâœ¨ Ready for production deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
